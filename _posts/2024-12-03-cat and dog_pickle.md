---
title:  "cat and dog - 실제 동작작"
categories: [머신러닝]
tags: [Blog, jekyll, Github, Git, 머신러닝]
date: 2024-12-03
last_modified_at: 2024-12-14
---
# 전에 training 시킨 모델을 가지고 실제로 이미지를 구별하는 부분
```python
import pickle
```


```python
X=pickle.load(open('X.pkl','rb'))
y=pickle.load(open('y.pkl','rb'))
```


```python
X
```




    array([[[[125, 123, 131],
             [129, 129, 129],
             [115, 125, 142],
             ...,
             [ 11,  28,  61],
             [ 13,  26,  60],
             [ 22,  35,  70]],
    
            [[101,  95, 105],
             [112, 111, 116],
             [ 70,  81, 101],
             ...,
             [  9,  28,  61],
             [ 11,  27,  63],
             [ 10,  28,  64]],
    
            [[ 59,  49,  58],
             [ 76,  75,  81],
             [ 15,  29,  56],
             ...,
             [  9,  23,  51],
             [ 10,  27,  57],
             [ 10,  26,  59]],
    
            ...,
    
            [[ 33,  37,  36],
             [ 30,  34,  33],
             [ 24,  28,  28],
             ...,
             [  3,  33,  73],
             [  0,  25,  68],
             [  9,  40, 100]],
    
            [[ 20,  24,  24],
             [ 14,  18,  17],
             [ 15,  19,  18],
             ...,
             [ 12,  46, 109],
             [ 13,  44,  98],
             [  2,  27,  64]],
    
            [[ 11,  15,  14],
             [ 11,  15,  14],
             [  8,  12,  11],
             ...,
             [  6,  36,  96],
             [ 19,  57, 113],
             [ 20,  56,  99]]],
    
    
           [[[ 87,  80,  28],
             [ 89,  82,  30],
             [ 72,  65,  13],
             ...,
             [ 14,  14,  12],
             [ 11,  11,   3],
             [ 14,  14,   7]],
    
            [[ 88,  81,  27],
             [ 81,  74,  20],
             [ 96,  89,  35],
             ...,
             [ 24,  20,  17],
             [ 13,  16,   7],
             [ 16,  12,   9]],
    
            [[ 87,  80,  25],
             [ 88,  81,  26],
             [ 92,  85,  30],
             ...,
             [ 27,  16,  12],
             [ 24,  25,  16],
             [ 26,  17,  15]],
    
            ...,
    
            [[138, 108,  80],
             [141, 115,  88],
             [158, 136, 112],
             ...,
             [142, 113,  83],
             [118,  89,  55],
             [147, 127,  90]],
    
            [[140, 114,  89],
             [155, 128, 103],
             [151, 127, 102],
             ...,
             [138, 107,  79],
             [161, 131, 101],
             [109,  88,  58]],
    
            [[145, 122,  95],
             [143, 113,  89],
             [153, 127, 102],
             ...,
             [127,  96,  68],
             [116,  85,  57],
             [130, 103,  79]]],
    
    
           [[[255, 255, 255],
             [255, 255, 255],
             [255, 255, 255],
             ...,
             [255, 255, 255],
             [255, 255, 255],
             [255, 255, 255]],
    
            [[255, 255, 255],
             [255, 255, 255],
             [255, 255, 255],
             ...,
             [255, 255, 255],
             [255, 255, 255],
             [255, 255, 255]],
    
            [[255, 255, 255],
             [255, 255, 255],
             [255, 255, 255],
             ...,
             [255, 255, 255],
             [255, 255, 255],
             [255, 255, 255]],
    
            ...,
    
            [[255, 255, 255],
             [255, 255, 255],
             [255, 255, 255],
             ...,
             [255, 255, 255],
             [255, 255, 255],
             [255, 255, 255]],
    
            [[255, 255, 255],
             [255, 255, 255],
             [255, 255, 255],
             ...,
             [255, 255, 255],
             [255, 255, 255],
             [255, 255, 255]],
    
            [[255, 255, 255],
             [255, 255, 255],
             [255, 255, 255],
             ...,
             [255, 255, 255],
             [255, 255, 255],
             [255, 255, 255]]],
    
    
           ...,
    
    
           [[[ 60,  69,  76],
             [ 72,  83,  99],
             [ 68,  81, 108],
             ...,
             [126, 139, 145],
             [124, 137, 146],
             [128, 131, 136]],
    
            [[ 47,  54,  61],
             [ 64,  73,  88],
             [ 72,  83, 104],
             ...,
             [133, 144, 148],
             [128, 138, 147],
             [131, 133, 145]],
    
            [[ 48,  53,  59],
             [ 66,  74,  84],
             [ 77,  87, 100],
             ...,
             [141, 151, 153],
             [126, 135, 142],
             [133, 136, 146]],
    
            ...,
    
            [[ 46,  46,  46],
             [ 55,  55,  55],
             [ 62,  62,  62],
             ...,
             [ 81,  80,  86],
             [ 75,  70,  77],
             [ 76,  72,  66]],
    
            [[ 55,  55,  57],
             [ 55,  55,  57],
             [ 50,  50,  51],
             ...,
             [ 81,  78,  85],
             [ 70,  63,  70],
             [ 73,  72,  68]],
    
            [[ 53,  53,  55],
             [ 54,  54,  56],
             [ 60,  60,  62],
             ...,
             [ 79,  76,  83],
             [ 63,  56,  63],
             [ 60,  59,  57]]],
    
    
           [[[177, 180, 159],
             [ 98, 100,  76],
             [101, 105,  75],
             ...,
             [110, 101, 104],
             [117, 108, 116],
             [ 99,  86,  96]],
    
            [[129, 129, 108],
             [ 91,  90,  65],
             [123, 121,  91],
             ...,
             [ 94,  84,  88],
             [ 82,  73,  76],
             [101,  89,  89]],
    
            [[185, 182, 157],
             [128, 122,  94],
             [132, 124,  94],
             ...,
             [143, 138, 164],
             [129, 125, 150],
             [ 98,  91, 113]],
    
            ...,
    
            [[108,  93,  89],
             [ 56,  39,  35],
             [ 49,  34,  29],
             ...,
             [113, 101,  95],
             [136, 124, 118],
             [118, 106, 100]],
    
            [[ 87,  68,  65],
             [ 54,  39,  37],
             [ 48,  36,  37],
             ...,
             [106, 100,  92],
             [105,  99,  91],
             [121, 116, 107]],
    
            [[ 60,  43,  45],
             [ 48,  39,  44],
             [ 39,  38,  45],
             ...,
             [167, 165, 156],
             [167, 166, 156],
             [173, 172, 162]]],
    
    
           [[[ 31,  35,  10],
             [ 33,  37,  12],
             [ 33,  37,  12],
             ...,
             [ 93,  97,  82],
             [ 92,  96,  81],
             [ 95,  99,  84]],
    
            [[ 32,  36,  11],
             [ 30,  34,   9],
             [ 33,  37,  12],
             ...,
             [ 92,  96,  81],
             [ 91,  95,  80],
             [ 95,  99,  84]],
    
            [[ 33,  37,  12],
             [ 33,  37,  12],
             [ 33,  37,  12],
             ...,
             [ 92,  96,  81],
             [ 91,  95,  80],
             [ 92,  96,  81]],
    
            ...,
    
            [[107, 114, 107],
             [110, 121, 113],
             [113, 129, 119],
             ...,
             [126, 139, 122],
             [116, 129, 112],
             [125, 131, 117]],
    
            [[103, 106,  99],
             [114, 122, 114],
             [117, 128, 121],
             ...,
             [127, 140, 123],
             [127, 140, 123],
             [128, 134, 120]],
    
            [[ 97, 100,  91],
             [105, 110, 103],
             [109, 118, 113],
             ...,
             [120, 133, 116],
             [124, 137, 120],
             [117, 123, 109]]]], dtype=uint8)




```python
y
```




    array([0, 1, 1, ..., 1, 1, 0])




```python
X=X/255
```


```python
X.shape
```




    (23000, 100, 100, 3)




```python
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
```


```python
model=Sequential()

model.add(Conv2D(64,(3,3),activation = 'relu'))
model.add(MaxPooling2D((2,2)))

model.add(Conv2D(64,(3,3),activation = 'relu'))
model.add(MaxPooling2D((2,2)))


model.add(Flatten())

model.add(Dense(128,input_shape = X.shape[1:],activation = 'relu'))

model.add(Dense(2,activation = 'softmax'))
```


```python
model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])
```

# 실제 파일로 돌릴 수 있음

## 파일의 개수는 25000개이지만, 아직  GPU 환경을 제대로 구성하지 못했기 때문에 CPU 기반으로 동작

## 이를 위해, CUDA 코어 사용하는 환경 구축 예정(2024.11.05)


```python
model.fit(X,y,epochs = 5, validation_split=0.1)
```

    Epoch 1/5
    647/647 [==============================] - 80s 123ms/step - loss: 0.6634 - accuracy: 0.6004 - val_loss: 0.6232 - val_accuracy: 0.6539
    Epoch 2/5
    647/647 [==============================] - 155s 240ms/step - loss: 0.5857 - accuracy: 0.6941 - val_loss: 0.5621 - val_accuracy: 0.7109
    Epoch 3/5
    647/647 [==============================] - 196s 303ms/step - loss: 0.4894 - accuracy: 0.7669 - val_loss: 0.5057 - val_accuracy: 0.7587
    Epoch 4/5
    647/647 [==============================] - 196s 304ms/step - loss: 0.3854 - accuracy: 0.8293 - val_loss: 0.5001 - val_accuracy: 0.7648
    Epoch 5/5
    647/647 [==============================] - 197s 305ms/step - loss: 0.2482 - accuracy: 0.9000 - val_loss: 0.5871 - val_accuracy: 0.7696
    




    <keras.callbacks.History at 0x1dd8a3160d0>




```python

```
